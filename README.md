# image-text-generation

Image captioning is becoming an increasingly relevant tool in todayâ€™s world. Text to speech processing has gained a lot of popularity for its convenience and image-to-speech processing is the next phase of furthering this convenience. It has many implications such as increasing safety for texting with Apple Carplay (by preventing people checking their phones to look at images) or increasing accessibility for visually-impaired people in reading articles or other image-related works. The main goal of our project is to build a model that, given an image, generates novel sentences describing the overall image as well as the relationships between objects. To do so, we implemented and compared the performances of three variations of encoder-decoder models. For the encoder, we used a pre-trained convolutional neural network (CNN) that extracts relevant features of provided images. For the decoder, we chose to experiment with the following three models: GRU, LSTM, and Transformer. In the end, we compared the BLEU score and validation loss of each model to evaluate the final results. With this project we hope to expand our understanding of the human language and contribute to the convenience and accessibility that image captioning brings.